{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assignment 3 - basic classifiers\n",
    "\n",
    "Math practice and coding application for main classifiers introduced in Chapter 3 of the Python machine learning book. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Weighting\n",
    "\n",
    "Note that this assignment is more difficult than the previous ones, and thus has a higher weighting 3 and longer duration (3 weeks). Each one of the previous two assignments has a weighting 1.\n",
    "\n",
    "Specifically, the first 3 assignments contribute to your continuous assessment as follows:\n",
    "\n",
    "Assignment weights: $w_1 = 1, w_2 = 1, w_3 = 3$\n",
    "\n",
    "Assignment grades: $g_1, g_2, g_3$\n",
    "\n",
    "Weighted average: $\\frac{1}{\\sum_i w_i} \\times \\sum_i \\left(w_i \\times g_i \\right)$\n",
    "\n",
    "Future assignments will be added analogously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBF kernel (20 points)\n",
    "\n",
    "Show that a Gaussian RBF kernel can be expressed as a dot product:\n",
    "$$\n",
    "K(\\mathbf{x}, \\mathbf{y}) \n",
    "= e^\\frac{-|\\mathbf{x} - \\mathbf{y}|^2}{2} \n",
    "= \\phi(\\mathbf{x})^T \\phi(\\mathbf{y})\n",
    "$$\n",
    "by spelling out the mapping function $\\phi$.\n",
    "\n",
    "For simplicity\n",
    "* you can assume both $\\mathbf{x}$ and $\\mathbf{y}$ are 2D vectors\n",
    "$\n",
    "x =\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{pmatrix}\n",
    ", \\;\n",
    "y =\n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\\n",
    "y_2\n",
    "\\end{pmatrix}\n",
    "$\n",
    "* we use a scalar unit variance here\n",
    "\n",
    "even though the proof can be extended for vectors $\\mathbf{x}$ $\\mathbf{y}$ and general covariance matrices.\n",
    "\n",
    "Hint: use Taylor series expansion of the exponential function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Answer:\n",
    "\n",
    "$$\n",
    "K(\\mathbf{x}, \\mathbf{y}) \n",
    "= e^\\frac{-|\\mathbf{x} - \\mathbf{y}|^2}{2} \n",
    "= e^\\frac{-||\\mathbf{x}||^2 - ||\\mathbf{y}||^2 + 2\\mathbf{x}^{T}\\mathbf{y}}{2} \n",
    "$$\n",
    "\n",
    "$$\n",
    "=exp\\left(\\frac{-||\\mathbf{x}||^2 - ||\\mathbf{y}||^2}{2}\\right) exp\\left(\\mathbf{x}^{T}\\mathbf{y}\\right)\n",
    "$$\n",
    "\n",
    "because $exp\\left(\\frac{-||\\mathbf{x}||^2 - ||\\mathbf{y}||^2}{2}\\right)$ is the constant that is not quite significant to the intepretation of Kernel function for proofing\n",
    "\n",
    "so basically we focus on the $exp\\left(\\mathbf{x}^{T}\\mathbf{y}\\right)$ this part\n",
    "\n",
    "Based on the tylor expansion:\n",
    "\n",
    "$$\n",
    "exp\\left(\\mathbf{x}^{T}\\mathbf{y}\\right) = 1 + \\frac{\\mathbf{x}^{T}\\mathbf{y}}{1\\,!}+\\frac{\\left(\\mathbf{x}^{T}\\mathbf{y}\\right)^{2}}{2\\,!}+\\frac{\\left(\\mathbf{x}^{T}\\mathbf{y}\\right)^{3}}{3\\,!}+...\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\sum_{t=1}^\\infty \\frac{(x_1y_1 + x_2y_2)^{t}}{t\\,!}\n",
    "$$\n",
    "\n",
    "by binomial expansion we get to know that\n",
    "\n",
    "$$\n",
    "(x_1y_1 + x_2y_2)^{n} = \\sum_i^{n} \\left((x_1y_1)^{n-i}(x_2y_2)^{i}\\right)\n",
    "$$\n",
    "\n",
    "so for every term in the taylor expansion i.e. $\\frac{(x_1y_1 + x_2y_2)^{t}}{t\\,!}$\n",
    "we can see it as $(t+1)$ terms inner product\n",
    "\n",
    "$$\n",
    "\\text{part of } \\phi(\\mathbf{X}) = \\phi^{(t)}(\\mathbf{X}) = \\frac{1}{\\sqrt{t\\,!}}\\left[\\binom t0(x_1)^{t}, \\binom t1(x_1)^{t-1}(x_2)^{1},...,\\binom t{t-1}(x_1)^{t-1}(x_2)^{1},\\binom tt(x_2)^{t}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{part of } \\phi(\\mathbf{Y}) = \\phi^{(t)}(\\mathbf{Y}) = \\frac{1}{\\sqrt{t\\,!}}\\left[\\binom t0(y_1)^{t}, \\binom t1(y_1)^{t-1}(y_2)^{1},...,\\binom t{t-1}(y_1)^{t-1}(y_2)^{1},\\binom tt(y_2)^{t}\\right]\n",
    "$$\n",
    "\n",
    "So we can express the transformation from $ \\mathbf{X}, \\mathbf{Y} $ using the token above\n",
    "\n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{X}) = \\phi((x_1,x_2)) = \\left[\\phi^{(0)}(\\mathbf{X}), \\phi^{(1)}(\\mathbf{X}), ... , \\phi^{(t)}(\\mathbf{X}), ..., \\phi^{(\\infty)}(\\mathbf{X})\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\left[1,\\frac{1}{\\sqrt{1\\,!}}\\binom 10x_1,\\frac{1}{\\sqrt{1\\,!}}\\binom 11x_2,...,\\frac{1}{\\sqrt{t\\,!}}\\binom t0(x_1)^{t}, \\frac{1}{\\sqrt{t\\,!}}\\binom t1(x_1)^{t-1}(x_2)^{1},...,\\frac{1}{\\sqrt{t\\,!}}\\binom t{t-1}(x_1)^{t-1}(x_2)^{1},\\frac{1}{\\sqrt{t\\,!}}\\binom tt(x_2)^{t},...\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{Y}) = \\phi((y_1,y_2)) = \\left[\\phi^{(0)}(\\mathbf{Y}), \\phi^{(1)}(\\mathbf{Y}), ... , \\phi^{(t)}(\\mathbf{Y}), ..., \\phi^{(\\infty)}(\\mathbf{Y})\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\left[1,\\frac{1}{\\sqrt{1\\,!}}\\binom 10y_1,\\frac{1}{\\sqrt{1\\,!}}\\binom 11y_2,...,\\frac{1}{\\sqrt{t\\,!}}\\binom t0(y_1)^{t}, \\frac{1}{\\sqrt{t\\,!}}\\binom t1(y_1)^{t-1}(y_2)^{1},...,\\frac{1}{\\sqrt{t\\,!}}\\binom t{t-1}(y_1)^{t-1}(y_2)^{1},\\frac{1}{\\sqrt{t\\,!}}\\binom tt(y_2)^{t},...\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel SVM complexity (10 points)\n",
    "\n",
    "How would the complexity (or number of parameters) of a kernel SVM change with the amount of training data, and why?\n",
    "Note that the answer may depend on the specific kernel used.\n",
    "Consider specifically the following types of kernels $K(\\mathbf{x}, \\mathbf{y})$.\n",
    "* linear:\n",
    "$$\n",
    "K\\left(\\mathbf{x}, \\mathbf{y}\\right) = \\mathbf{x}^T \\mathbf{y}\n",
    "$$\n",
    "* polynomial with degree $q$:\n",
    "$$\n",
    "K\\left(\\mathbf{x}, \\mathbf{y}\\right) =\n",
    "(\\mathbf{x}^T\\mathbf{y} + 1)^q\n",
    "$$\n",
    "* RBF with distance function $D$:\n",
    "$$\n",
    "K\\left(\\mathbf{x}, \\mathbf{y} \\right) = e^{-\\frac{D\\left(\\mathbf{x}, \\mathbf{y} \\right)}{2s^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Answer:\n",
    "\n",
    "At frist we make some assumption:\n",
    "1. the time of conduction a product operation use constant time $c_p$\n",
    "2. the time of conduction a division operation use time $c_d$\n",
    "3. the time of conduction a addition or subtraction operation use time $c$\n",
    "\n",
    "\n",
    "\n",
    "$\\textit{1}$. For linear kernel\n",
    "\n",
    "the kernel map from a vector with $ \\textit{d}$ dimention to $ \\textit{d}$\n",
    "typora\n",
    "then for every dimention we need to perform one product on the kernel function\n",
    "\n",
    "So the complexity of the linear kernel is\n",
    "\n",
    "$$\n",
    "Complexity(\\text{linearKernel}) = O(dnc_p) = O(dn)\n",
    "$$\n",
    "\n",
    "\n",
    "$\\textit{2}$. For polynomial kernel with degree q\n",
    "\n",
    "the kernel map from a vector with $ \\textit{d}$ dimention to $ \\binom {q+d-1}{q}$\n",
    "\n",
    "The complexity of the linear kernel is\n",
    "\n",
    "$$\n",
    "Complexity(\\text{polynomialKernel}) = O(n(dc_p + c_e + c+logq)) = O(n(d+logq))\n",
    "$$\n",
    "(Note here we assume that exponential calculation has complexity of O(logn) where n is the degree)\n",
    "\n",
    "$\\textit{3}$. For RBF with distance function $D$:\n",
    "\n",
    "the kernel map from a vector with $ \\textit{d}$ dimention to infinity (as the first question shows)\n",
    "\n",
    "The complexity of the linear kernel is\n",
    "\n",
    "(Note that the $s$ has been decided by the argument in svm of $\\gamma$ so we do not need to calculate it)\n",
    "\n",
    "$$\n",
    "Complexity(\\text{rbfKernel}) = O(dn + dn + 4c_e + Xc) = O(dn) \n",
    "$$\n",
    "(where X is a constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gaussian density Bayes (30 points)\n",
    "\n",
    "$$\n",
    "p\\left(\\Theta | \\mathbf{X}\\right)\n",
    "= \n",
    "\\frac{p\\left(\\mathbf{X} | \\Theta\\right) p\\left(\\Theta\\right)}{p\\left(\\mathbf{X}\\right)}\n",
    "$$\n",
    "\n",
    "Assume both the likelihood and prior have Gaussian distributions:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathbf{X} | \\Theta)\n",
    "&=\n",
    "\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right)\n",
    "\\\\\n",
    "p(\\Theta)\n",
    "&=\n",
    "\\frac{1}{\\sqrt{2\\pi}\\sigma_0} \\exp\\left( -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Derive $\\Theta$ from the dataset $\\mathbf{X}$ via the following methods:\n",
    "\n",
    "### ML (maximum likelihood) estimation \n",
    "$$\n",
    "\\Theta_{ML} = argmax_{\\Theta} p(\\mathbf{X} | \\Theta)\n",
    "$$\n",
    "\n",
    "### MAP estimation\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Theta_{MAP} \n",
    "&= \n",
    "argmax_{\\Theta} p(\\Theta | \\mathbf{X})\n",
    "\\\\\n",
    "&=\n",
    "argmax_{\\Theta} p(\\mathbf{X} | \\Theta) p(\\Theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Bayes estimation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Theta_{Bayes} \n",
    "&= \n",
    "E(\\Theta | \\mathbf{X})\n",
    "\\\\\n",
    "&= \n",
    "\\int \\Theta p(\\Theta | \\mathbf{X}) d\\Theta\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Answer:\n",
    "\n",
    "$\\textit{1}$.Using ML (maximum likelihood) estimation to get $ \\mathbf{\\Theta} $\n",
    "$$\n",
    "\\Theta_{ML} = argmax_{\\Theta} p(\\mathbf{X} | \\Theta)\n",
    "$$\n",
    "First we find the derivative of $p(\\mathbf{X} | \\Theta)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial p(\\mathbf{X} | \\Theta)}{\\partial \\Theta} \n",
    "&= \\frac{\\partial \\left(\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right)\\right)}{\\partial \\Theta}\n",
    "\\\\\n",
    "&=\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\frac{\\partial \\left(\\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right)\\right)}{\\partial \\Theta}\n",
    "\\\\\n",
    "&=\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\left(\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)}{\\sigma^2}\\right) \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right)\n",
    "\\\\\n",
    "&=\\frac{1}{(2\\pi)^{N/2}\\sigma^N}\\left(\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)}) - N\\Theta}{\\sigma^2}\\right) \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then we want to maximize $p(\\mathbf{X} | \\Theta)$ just let the derivative to be zero\n",
    "which is\n",
    "$$\n",
    "\\frac{1}{(2\\pi)^{N/2}\\sigma^N}\\left(\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)}) - N\\Theta}{\\sigma^2}\\right) \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right) = 0\n",
    "$$\n",
    "Then, because \n",
    "$$\n",
    " \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right) > 0\n",
    "$$\n",
    "\n",
    "So\n",
    "\n",
    "$$\n",
    "\\sum_{t=1}^N (\\mathbf{x}^{(t)}) - N\\Theta = 0\n",
    "$$\n",
    "\n",
    "So we got\n",
    "\n",
    "$$\n",
    "\\Theta = \\frac{\\sum_{t=1}^N \\mathbf{x}^{(t)}}{N}\n",
    "$$\n",
    "\n",
    "$\\textit{2}.$Using MAP (maxium a postprior) estimation to get $ \\mathbf{\\Theta} $\n",
    "\n",
    "Similar to the ML method, we firstly try to get the derivative of the expression ${\\Theta} p(\\Theta | \\mathbf{X})$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\left(p(\\Theta) p(\\mathbf{X} | \\Theta)\\right)}{\\partial \\Theta} \n",
    "&= \\frac{\\partial \\left(\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right) \\frac{1}{\\sqrt{2\\pi}\\sigma_0} \\exp\\left( -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\\right)}{\\partial \\Theta}\n",
    "\\\\\n",
    "&=\\frac{1}{(2\\pi)^{N/2}\\sigma^N \\sqrt{2\\pi}\\sigma_0}\\frac{\\partial \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}  -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2}\\right) }{\\partial \\Theta}\n",
    "\\\\\n",
    "&=\\frac{1}{(2\\pi)^{N/2}\\sigma^N \\sqrt{2\\pi}\\sigma_0}\\left(\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)}{\\sigma^2}  -\\frac{(\\Theta - \\mu_0)}{\\sigma_0^2}\\right)\\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}  -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Then we want to maximize $p(\\mathbf{X} | \\Theta)$ just let the derivative to be zero\n",
    "which is\n",
    "\n",
    "$$\n",
    "\\frac{1}{(2\\pi)^{N/2}\\sigma^N \\sqrt{2\\pi}\\sigma_0}\\left(\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)}{\\sigma^2}  -\\frac{(\\Theta - \\mu_0)}{\\sigma_0^2}\\right)\\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}  -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2}\\right) = 0\n",
    "$$\n",
    "\n",
    "Then, because \n",
    "\n",
    "$$\n",
    "\\exp\\left(\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}  -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2}\\right) > 0\n",
    "$$\n",
    "\n",
    "So \n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)}{\\sigma^2}  -\\frac{(\\Theta - \\mu_0)}{\\sigma_0^2} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_0^2(\\sum_{t=1}^N \\mathbf{x}^{(t)} - N\\Theta) -\\sigma^2(\\Theta - \\mu_0) = 0\n",
    "$$\n",
    "\n",
    "So we got\n",
    "\n",
    "$$\n",
    "\\Theta =\n",
    "\\frac{N/\\sigma^2}{N/\\sigma^2 + 1/\\sigma_0^2} \\frac{\\sum_{t=1}^N \\mathbf{x}^{(t)}}{N}\n",
    "+\n",
    "\\frac{1/\\sigma_0^2}{N/\\sigma^2 + 1/\\sigma_0^2} \\mu_0\n",
    "$$\n",
    "\n",
    "\n",
    "$\\textit{3}$.Using Bayes estimation to get $ \\mathbf{\\Theta} $\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Theta \n",
    "&= \n",
    "E(\\Theta | \\mathbf{X})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Intuitively, we need to find the distribution with respect to the random variable $(\\Theta | \\mathbf{X}) $\n",
    "\n",
    "by derving to pdf(probability density function) of the distribution we can find the mean which is the $\\Theta$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "pdf(\\Theta | \\mathbf{X}) \n",
    "&= p(\\Theta | \\mathbf{X})\n",
    "\\\\\n",
    "&= \\frac{p\\left(\\mathbf{X} | \\Theta\\right) p\\left(\\Theta\\right)}{p\\left(\\mathbf{X}\\right)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Becuase $\\mathbf{X}$ is already known (the data set), and it's not related to the distribution of the $\\Theta$ because we need to find the $\\Theta$\n",
    "\n",
    "So consider the following pdf\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p\\left(\\mathbf{X} | \\Theta\\right) p\\left(\\Theta\\right)\n",
    "&=\\left(\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}\\right) \\frac{1}{\\sqrt{2\\pi}\\sigma_0} \\exp\\left( -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2} \\right)\\right)\n",
    "\\\\\n",
    "&=\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\frac{1}{\\sqrt{2\\pi}\\sigma_0}\\exp\\left(-\\frac{\\sum_{t=1}^N (\\mathbf{x}^{(t)} - \\Theta)^2}{2\\sigma^2}  -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2}\\right)  \n",
    "\\\\\n",
    "&=\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\frac{1}{\\sqrt{2\\pi}\\sigma_0}\\exp\\left(-\\frac{\\sum_{t=1}^N (\\left(\\mathbf{x}^{(t)}\\right)^{2}) - 2\\Theta\\sum_{t=1}^N (\\left(\\mathbf{x}^{(t)}\\right)) + N\\Theta^{2}}{2\\sigma^2}  -\\frac{(\\Theta - \\mu_0)^2}{2\\sigma_0^2}\\right)  \n",
    "\\\\\n",
    "&=\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\frac{1}{\\sqrt{2\\pi}\\sigma_0}\\exp\\left(-\\frac{\\sigma_0^2\\sum_{t=1}^N (\\left(\\mathbf{x}^{(t)}\\right)^{2}) - 2\\sigma_0^2\\Theta\\sum_{t=1}^N (\\left(\\mathbf{x}^{(t)}\\right)) + \\sigma_0^2N\\Theta^{2} + \\sigma^2\\Theta^2 -2\\sigma^2\\Theta\\mu_0 + \\mu_0^2\\sigma^2 }{2\\sigma^2\\sigma_0^2}\\right)  \n",
    "\\\\\n",
    "&=\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\frac{1}{\\sqrt{2\\pi}\\sigma_0}\\exp\\left(-\\frac{1}{2}\\left(\\left(\\frac{1}{\\sigma_0^2}+\\frac{N}{\\sigma^2}\\right)\\Theta^2 - 2\\left(\\frac{\\sum_{t=1}^N\\mathbf{X}^{(t)}}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right)\\Theta + \\left(\\frac{\\sum_{t=1}^N(\\mathbf{X}^{(t)})^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\sigma_0^2}\\right)\\right)\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "here we may denote \n",
    "$\n",
    "\\mathbf{E} = \\frac{\\sum_{t=1}^N\\mathbf{X}^{(t)}}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\n",
    "$\n",
    "for easy of expression\n",
    "\n",
    "So the expression above become \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "&=\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\frac{1}{\\sqrt{2\\pi}\\sigma_0}\\exp\\left(-\\frac{1}{2} \\left(\\frac{\\sum_{t=1}^N(\\mathbf{X}^{(t)})^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\sigma_0^2}\\right)\\right)\\exp\\left(-\\frac{1}{2}\\left(\\left(\\frac{1}{\\sigma_0^2}+\\frac{N}{\\sigma^2}\\right)\\Theta^2 - 2\\mathbf{E}\\Theta\\right)\\right)\n",
    "\\\\\n",
    "&=\\frac{1}{(2\\pi)^{N/2}\\sigma^N} \\frac{1}{\\sqrt{2\\pi}\\sigma_0}\\exp\\left(-\\frac{1}{2}\\left(\\frac{\\sum_{t=1}^N(\\mathbf{X}^{(t)})^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\sigma_0^2}-\\frac{\\mathbf{E}^{2}}{\\left(\\frac{1}{\\sigma_0^2}+\\frac{N}{\\sigma^2}\\right)^{2}}\\right)\\right)\\exp\\left(-\\frac{\\left(\\Theta - \\frac{\\mathbf{E}}{\\left(\\frac{1}{\\sigma_0^2}+\\frac{N}{\\sigma^2}\\right)}\\right)^{2}}{2\\frac{\\mu^2\\mu_0^2}{\\mu^2+N\\mu_0^2}}\\right)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "we can find that it's a scaled normal distribution with a constant which however does not change its means\n",
    "\n",
    "$$\n",
    "E(\\Theta | \\mathbf{X}) = \\text{means of pdf()} = \\frac{\\mathbf{E}}{\\left(\\frac{1}{\\sigma_0^2}+\\frac{N}{\\sigma^2}\\right)}\n",
    "$$\n",
    "\n",
    "then we plug back to the expression $\\mathbf{E}$\n",
    "So we got the result same as the second method using method MAP\n",
    "\n",
    "$$\n",
    "\\Theta =\n",
    "\\frac{N/\\sigma^2}{N/\\sigma^2 + 1/\\sigma_0^2} \\frac{\\sum_{t=1}^N \\mathbf{x}^{(t)}}{N}\n",
    "+\n",
    "\\frac{1/\\sigma_0^2}{N/\\sigma^2 + 1/\\sigma_0^2} \\mu_0\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hand-written digit classification (40 points)\n",
    "\n",
    "In the textbook sample code we applied different scikit-learn classifers for the Iris data set.\n",
    "\n",
    "In this exercise, we will apply the same set of classifiers over a different data set: hand-written digits.\n",
    "Please write down the code for different classifiers, choose their hyper-parameters, and compare their performance via the accuracy score as in the Iris dataset.\n",
    "Which classifier(s) perform(s) the best and worst, and why?\n",
    "\n",
    "The classifiers include:\n",
    "* perceptron\n",
    "* logistic regression\n",
    "* SVM\n",
    "* decision tree\n",
    "* random forest\n",
    "* KNN\n",
    "* naive Bayes\n",
    "\n",
    "The dataset is available as part of scikit learn, as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.data # training data\n",
    "y = digits.target # training label\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef69fa02e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAFsCAYAAACJh6H7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUiUWrCRVYsFKUiX/D2PfpQakafihAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef69ebd7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pylab as pl\n",
    "\n",
    "index = 13\n",
    "pl.gray()\n",
    "pl.matshow(digits.images[index])\n",
    "pl.title('digit ' + str(digits.target[index]))\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Date Preprocessing\n",
    "Hint: How you divide training and test data set? And apply other techinques we have learned if needed.\n",
    "You could take a look at the Iris data set case in the textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Your code comes here\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "# splitting data into 80% training and 20% test data: \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "#here we want to test the c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #1 Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 22 out of 360\n",
      "Accuracy: 0.9389\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "perceptron = Perceptron(n_iter=10, verbose=0, fit_intercept=True, eta0=0.1, random_state=0, penalty = 'l2', alpha = 0.1**4)\n",
    "perceptron.fit(X_train_std, y_train)\n",
    "y_pred = perceptron.predict(X_test_std)\n",
    "print('Misclassified samples: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "print('Accuracy: %.4f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 13 out of 360\n",
      "Accuracy: 0.9639\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "# test the influence of C (regularization strength)\n",
    "\n",
    "logisticRegression = LogisticRegression(C=10, random_state=0)\n",
    "logisticRegression.fit(X_train_std, y_train)\n",
    "y_pred = logisticRegression.predict(X_test_std)\n",
    "print('Misclassified samples: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "print('Accuracy: %.4f' % accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #3 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 4 out of 360\n",
      "Accuracy: 0.9889\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "svm = SVC(kernel='poly', C=10, random_state=0)\n",
    "svm.fit(X_train_std, y_train)\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print('Misclassified samples: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "print('Accuracy: %.4f' % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #4 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 41 out of 360\n",
      "Accuracy: 0.8861\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decisionTree = DecisionTreeClassifier(criterion=\"entropy\",random_state=0, max_depth = 10)\n",
    "decisionTree.fit(X_train_std, y_train)\n",
    "y_pred = decisionTree.predict(X_test_std)\n",
    "print('Misclassified samples: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "print('Accuracy: %.4f' % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifer #5 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 10 out of 360\n",
      "Accuracy: 0.9722\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "randomForest = RandomForestClassifier(criterion='gini',\n",
    "                                n_estimators=50, \n",
    "                                random_state=0,\n",
    "                                n_jobs=-1)\n",
    "randomForest.fit(X_train_std, y_train)\n",
    "y_pred = randomForest.predict(X_test_std)\n",
    "print('Misclassified samples: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "print('Accuracy: %.4f' % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #6 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 5 out of 360\n",
      "Accuracy: 0.9861\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski', algorithm = 'auto')\n",
    "knn.fit(X_train_std, y_train)\n",
    "y_pred = knn.predict(X_test_std)\n",
    "print('Misclassified samples: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "print('Accuracy: %.4f' % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Classifier #7 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 46 out of 360\n",
      "Accuracy: 0.8722\n"
     ]
    }
   ],
   "source": [
    "#Your code, including traing and testing, to observe the accuracies.\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_std, y_train)\n",
    "y_pred = bnb.predict(X_test_std)\n",
    "print('Misclassified samples: %d out of %d' % ((y_test != y_pred).sum(), y_test.shape[0]))\n",
    "print('Accuracy: %.4f' % accuracy_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Analysis:\n",
    "\n",
    "Here is the result for the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.988889</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.986111</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.972222</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.963889</td>\n",
       "      <td>logistic regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.938889</td>\n",
       "      <td>perceptron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.886111</td>\n",
       "      <td>decision tree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.872222</td>\n",
       "      <td>naive Bayes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy           classifier\n",
       "2  0.988889                  SVM\n",
       "5  0.986111                  KNN\n",
       "4  0.972222        random forest\n",
       "1  0.963889  logistic regression\n",
       "0  0.938889           perceptron\n",
       "3  0.886111        decision tree\n",
       "6  0.872222          naive Bayes"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "classifiersName =[\"perceptron\", \"logistic regression\", \"SVM\", \"decision tree\", \"random forest\", \"KNN\", \"naive Bayes\"]\n",
    "classifiers = [perceptron, logisticRegression, svm, decisionTree, randomForest, knn, bnb]\n",
    "score = [accuracy_score(y_test, i.predict(X_test_std)) for i in classifiers]\n",
    "combineResults = []\n",
    "for i in range(7):\n",
    "    combineResults.append({'classifier' : classifiersName[i], 'accuracy' :score[i]})\n",
    "\n",
    "df = DataFrame(data=combineResults)\n",
    "df.sort_values(by='accuracy', ascending=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through at least 10 tests, we can find that SVM are have obvious advantage in terms of high accuracy.\n",
    "This is mainly because SVM are better dealing with clustering data when all the data are close with each other in the same sets and different sets are comparatively far away from each other.\n",
    "\n",
    "And as for the worst model for this dataset, decision tree may be the one. The reason could be in the following that:\n",
    "1. Decision tree treat every feature useful where the outer pixel maybe always has low value in gray scale and they are usually of no use for classfication.\n",
    "2. Decision tree once only take one feature to split data which may ignore some combinational relationship (like two feature combined has better impact on the spliting)\n",
    "3. Sometimes decision tree without smart pruning can be overfitting with low accuracy in terms of test case.\n",
    "4. Decision tree only care about information gain rather than class. For example, if there is two ways to split current data into two sets, two have close information gain, it is possible that the one having a bit lower information gain could benefit next spliting more than the one with higher infomation gain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
